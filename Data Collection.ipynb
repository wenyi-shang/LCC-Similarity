{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f44cea-d078-451d-b53d-eeb8214cf78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Before running the code, you will need to download the txt-files.tar.zip \n",
    "# file from https://www.gutenberg.org/cache/epub/feeds/\n",
    "# note: The version of txt-files.tar.zip used for this study was last modified on 2025-03-09 19:18\n",
    "# note: the full text files are inside of a folder titled epub, but all of the files are .txt files\n",
    "# note: this file is too large to store in a GitHub repository. It has to be downloaded manually. The\n",
    "# zip file is 9.9gb, and unzipped the texts are 28.8gb.\n",
    "\n",
    "# 2. Once downloaded and unzipped, update your directory structure. mine looks like:\n",
    "# \n",
    "# wd/data/\n",
    "#     ├── pg-texts/\n",
    "#     │   ├── 1/\n",
    "#     │   │   ├── pg1.txt\n",
    "#     │   ├── 2/\n",
    "#     │   │   ├── pg2.txt\n",
    "# \n",
    "# E.g. wd/data/pg-texts/1/pg1.txt, instead of wd/data/cache/epub/1/pg1.txt\n",
    "# a) the \"epub\" folder is renamed to \"pg-texts\", and b) \"pg-texts\" is moved\n",
    "# up a level in the hierarchy, i.e. delete the \"cache\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a6eea5-ef5e-4ac9-afdc-cf069d45d3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae317f1-2b89-4e6d-abd9-cefc673ed6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Project Gutenberg (PG) CSV metadata (download from https://www.gutenberg.org/cache/epub/feeds/)\n",
    "# The file used for this study was last modified on 2025-02-16 17:04\n",
    "\n",
    "data = \"/path/to/pg_catalog.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5a67f64-b28d-42d6-be76-c6d7c36524e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PG metadata into dataframe\n",
    "df = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d992a58-21ba-4cc4-a9c0-44834c096e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some texts have multiple Library of Congress classification labels, so we want to split these into\n",
    "# a separate column to have a more accurate count. Otherwise, the data will contain instances \n",
    "# of double classification (e.g. \"AB; AC\")\n",
    "# Note: this means these texts will be analyzed twice. that is, a text with \"AB; AC\"\n",
    "# will appear in both the AB and AC dataframes\n",
    "\n",
    "# Parse the LoCC colum. This will put anything after the first semicolon into a new column (\"LoCC2\")\n",
    "# And then parses LoCC2, putting anything after the first semicolon into a new column (\"LoCC3\"), etc, etc\n",
    "\n",
    "locc_split = df[\"LoCC\"].astype(str).str.split(\";\", expand=True)\n",
    "\n",
    "# Rename the columns: \"LoCC\" for the first label, then \"LoCC2\", \"LoCC3\", etc.\n",
    "df[\"LoCC\"] = locc_split[0]  # Update \"LoCC\" to contain only the first label\n",
    "df[[\"LoCC2\", \"LoCC3\", \"LoCC4\", \"LoCC5\"]] = locc_split.iloc[:, 1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "662b8fd9-6d90-4986-a416-423c5231afd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text#</th>\n",
       "      <th>Type</th>\n",
       "      <th>Issued</th>\n",
       "      <th>Title</th>\n",
       "      <th>Language</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Subjects</th>\n",
       "      <th>LoCC</th>\n",
       "      <th>Bookshelves</th>\n",
       "      <th>LoCC2</th>\n",
       "      <th>LoCC3</th>\n",
       "      <th>LoCC4</th>\n",
       "      <th>LoCC5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>1971-12-01</td>\n",
       "      <td>The Declaration of Independence of the United ...</td>\n",
       "      <td>en</td>\n",
       "      <td>Jefferson, Thomas, 1743-1826</td>\n",
       "      <td>United States -- History -- Revolution, 1775-1...</td>\n",
       "      <td>E201</td>\n",
       "      <td>Politics; American Revolutionary War; United S...</td>\n",
       "      <td>JK</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Text</td>\n",
       "      <td>1972-12-01</td>\n",
       "      <td>The United States Bill of Rights\\r\\nThe Ten Or...</td>\n",
       "      <td>en</td>\n",
       "      <td>United States</td>\n",
       "      <td>Civil rights -- United States -- Sources; Unit...</td>\n",
       "      <td>JK</td>\n",
       "      <td>Politics; American Revolutionary War; United S...</td>\n",
       "      <td>KF</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Text</td>\n",
       "      <td>1973-11-01</td>\n",
       "      <td>John F. Kennedy's Inaugural Address</td>\n",
       "      <td>en</td>\n",
       "      <td>Kennedy, John F. (John Fitzgerald), 1917-1963</td>\n",
       "      <td>United States -- Foreign relations -- 1961-196...</td>\n",
       "      <td>E838</td>\n",
       "      <td>Browsing: History - American; Browsing: Politics</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Text</td>\n",
       "      <td>1973-11-01</td>\n",
       "      <td>Lincoln's Gettysburg Address\\r\\nGiven November...</td>\n",
       "      <td>en</td>\n",
       "      <td>Lincoln, Abraham, 1809-1865</td>\n",
       "      <td>Consecration of cemeteries -- Pennsylvania -- ...</td>\n",
       "      <td>E456</td>\n",
       "      <td>US Civil War; Browsing: History - American; Br...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Text</td>\n",
       "      <td>1975-12-01</td>\n",
       "      <td>The United States Constitution</td>\n",
       "      <td>en</td>\n",
       "      <td>United States</td>\n",
       "      <td>United States -- Politics and government -- 17...</td>\n",
       "      <td>JK</td>\n",
       "      <td>United States; Politics; American Revolutionar...</td>\n",
       "      <td>KF</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Text#  Type      Issued                                              Title  \\\n",
       "0      1  Text  1971-12-01  The Declaration of Independence of the United ...   \n",
       "1      2  Text  1972-12-01  The United States Bill of Rights\\r\\nThe Ten Or...   \n",
       "2      3  Text  1973-11-01                John F. Kennedy's Inaugural Address   \n",
       "3      4  Text  1973-11-01  Lincoln's Gettysburg Address\\r\\nGiven November...   \n",
       "4      5  Text  1975-12-01                     The United States Constitution   \n",
       "\n",
       "  Language                                        Authors  \\\n",
       "0       en                   Jefferson, Thomas, 1743-1826   \n",
       "1       en                                  United States   \n",
       "2       en  Kennedy, John F. (John Fitzgerald), 1917-1963   \n",
       "3       en                    Lincoln, Abraham, 1809-1865   \n",
       "4       en                                  United States   \n",
       "\n",
       "                                            Subjects  LoCC  \\\n",
       "0  United States -- History -- Revolution, 1775-1...  E201   \n",
       "1  Civil rights -- United States -- Sources; Unit...    JK   \n",
       "2  United States -- Foreign relations -- 1961-196...  E838   \n",
       "3  Consecration of cemeteries -- Pennsylvania -- ...  E456   \n",
       "4  United States -- Politics and government -- 17...    JK   \n",
       "\n",
       "                                         Bookshelves LoCC2 LoCC3 LoCC4 LoCC5  \n",
       "0  Politics; American Revolutionary War; United S...    JK  None  None  None  \n",
       "1  Politics; American Revolutionary War; United S...    KF  None  None  None  \n",
       "2   Browsing: History - American; Browsing: Politics  None  None  None  None  \n",
       "3  US Civil War; Browsing: History - American; Br...  None  None  None  None  \n",
       "4  United States; Politics; American Revolutionar...    KF  None  None  None  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccf03a28-f4a9-4468-8784-eec5180c7e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the split worked; LoCC4 should have 4 rows with data; and we should also be able to \n",
    "# see the 3 rows with in LoCC5 that contain data\n",
    "\n",
    "df_sorted_alphabetical = df.sort_values(by=\"LoCC4\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b317e728-4f8c-4064-8a0f-05f3373949d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text#</th>\n",
       "      <th>Type</th>\n",
       "      <th>Issued</th>\n",
       "      <th>Title</th>\n",
       "      <th>Language</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Subjects</th>\n",
       "      <th>LoCC</th>\n",
       "      <th>Bookshelves</th>\n",
       "      <th>LoCC2</th>\n",
       "      <th>LoCC3</th>\n",
       "      <th>LoCC4</th>\n",
       "      <th>LoCC5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12195</th>\n",
       "      <td>12266</td>\n",
       "      <td>Text</td>\n",
       "      <td>2004-05-01</td>\n",
       "      <td>Von Haparanda bis San Francisco: Reise-Erinner...</td>\n",
       "      <td>de</td>\n",
       "      <td>Wasserzieher, Ernst, 1860-1927</td>\n",
       "      <td>Germany -- Description and travel; United Stat...</td>\n",
       "      <td>DD</td>\n",
       "      <td>United States; DE Sachbuch; Browsing: History ...</td>\n",
       "      <td>DK</td>\n",
       "      <td>DL</td>\n",
       "      <td>E151</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6280</th>\n",
       "      <td>6322</td>\n",
       "      <td>Text</td>\n",
       "      <td>2004-08-01</td>\n",
       "      <td>Personal Narrative of Travels to the Equinocti...</td>\n",
       "      <td>en</td>\n",
       "      <td>Humboldt, Alexander von, 1769-1859; Bonpland, ...</td>\n",
       "      <td>South America -- Description and travel; Voyag...</td>\n",
       "      <td>G</td>\n",
       "      <td>South America; Browsing: History - General; Br...</td>\n",
       "      <td>QE</td>\n",
       "      <td>QH</td>\n",
       "      <td>F2201</td>\n",
       "      <td>F2301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22421</th>\n",
       "      <td>22492</td>\n",
       "      <td>Text</td>\n",
       "      <td>2007-09-03</td>\n",
       "      <td>Reise in die Aequinoctial-Gegenden des neuen C...</td>\n",
       "      <td>de</td>\n",
       "      <td>Humboldt, Alexander von, 1769-1859; Hauff, Her...</td>\n",
       "      <td>South America -- Description and travel; Geolo...</td>\n",
       "      <td>G</td>\n",
       "      <td>German Language Books; DE Sachbuch; Browsing: ...</td>\n",
       "      <td>QE</td>\n",
       "      <td>QH</td>\n",
       "      <td>F2201</td>\n",
       "      <td>F2301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24675</th>\n",
       "      <td>24746</td>\n",
       "      <td>Text</td>\n",
       "      <td>2008-03-03</td>\n",
       "      <td>Reise in die Aequinoctial-Gegenden des neuen C...</td>\n",
       "      <td>de</td>\n",
       "      <td>Humboldt, Alexander von, 1769-1859; Hauff, Her...</td>\n",
       "      <td>South America -- Description and travel; Geolo...</td>\n",
       "      <td>G</td>\n",
       "      <td>DE Sachbuch; Browsing: History - General; Brow...</td>\n",
       "      <td>QE</td>\n",
       "      <td>QH</td>\n",
       "      <td>F2201</td>\n",
       "      <td>F2301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Text</td>\n",
       "      <td>1971-12-01</td>\n",
       "      <td>The Declaration of Independence of the United ...</td>\n",
       "      <td>en</td>\n",
       "      <td>Jefferson, Thomas, 1743-1826</td>\n",
       "      <td>United States -- History -- Revolution, 1775-1...</td>\n",
       "      <td>E201</td>\n",
       "      <td>Politics; American Revolutionary War; United S...</td>\n",
       "      <td>JK</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75210</th>\n",
       "      <td>75386</td>\n",
       "      <td>Text</td>\n",
       "      <td>2025-02-16</td>\n",
       "      <td>Kertomuksia ja kuvauksia elämästä</td>\n",
       "      <td>fi</td>\n",
       "      <td>Kataja, Liina</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75211</th>\n",
       "      <td>75387</td>\n",
       "      <td>Text</td>\n",
       "      <td>2025-02-16</td>\n",
       "      <td>Kohtalon leikkiä</td>\n",
       "      <td>fi</td>\n",
       "      <td>Röösgrén, Liina</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75212</th>\n",
       "      <td>75388</td>\n",
       "      <td>Text</td>\n",
       "      <td>2025-02-16</td>\n",
       "      <td>Sommerleutnants</td>\n",
       "      <td>de</td>\n",
       "      <td>Bloem, Walter, 1868-1951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75213</th>\n",
       "      <td>75389</td>\n",
       "      <td>Text</td>\n",
       "      <td>2025-02-16</td>\n",
       "      <td>Principles and practice of agricultural analysis.</td>\n",
       "      <td>en</td>\n",
       "      <td>Wiley, Harvey Washington, 1844-1930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75214</th>\n",
       "      <td>90907</td>\n",
       "      <td>Text</td>\n",
       "      <td>2019-12-12</td>\n",
       "      <td>No title</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75215 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Text#  Type      Issued  \\\n",
       "12195  12266  Text  2004-05-01   \n",
       "6280    6322  Text  2004-08-01   \n",
       "22421  22492  Text  2007-09-03   \n",
       "24675  24746  Text  2008-03-03   \n",
       "0          1  Text  1971-12-01   \n",
       "...      ...   ...         ...   \n",
       "75210  75386  Text  2025-02-16   \n",
       "75211  75387  Text  2025-02-16   \n",
       "75212  75388  Text  2025-02-16   \n",
       "75213  75389  Text  2025-02-16   \n",
       "75214  90907  Text  2019-12-12   \n",
       "\n",
       "                                                   Title Language  \\\n",
       "12195  Von Haparanda bis San Francisco: Reise-Erinner...       de   \n",
       "6280   Personal Narrative of Travels to the Equinocti...       en   \n",
       "22421  Reise in die Aequinoctial-Gegenden des neuen C...       de   \n",
       "24675  Reise in die Aequinoctial-Gegenden des neuen C...       de   \n",
       "0      The Declaration of Independence of the United ...       en   \n",
       "...                                                  ...      ...   \n",
       "75210                  Kertomuksia ja kuvauksia elämästä       fi   \n",
       "75211                                   Kohtalon leikkiä       fi   \n",
       "75212                                    Sommerleutnants       de   \n",
       "75213  Principles and practice of agricultural analysis.       en   \n",
       "75214                                           No title       en   \n",
       "\n",
       "                                                 Authors  \\\n",
       "12195                     Wasserzieher, Ernst, 1860-1927   \n",
       "6280   Humboldt, Alexander von, 1769-1859; Bonpland, ...   \n",
       "22421  Humboldt, Alexander von, 1769-1859; Hauff, Her...   \n",
       "24675  Humboldt, Alexander von, 1769-1859; Hauff, Her...   \n",
       "0                           Jefferson, Thomas, 1743-1826   \n",
       "...                                                  ...   \n",
       "75210                                      Kataja, Liina   \n",
       "75211                                    Röösgrén, Liina   \n",
       "75212                           Bloem, Walter, 1868-1951   \n",
       "75213                Wiley, Harvey Washington, 1844-1930   \n",
       "75214                                                NaN   \n",
       "\n",
       "                                                Subjects  LoCC  \\\n",
       "12195  Germany -- Description and travel; United Stat...    DD   \n",
       "6280   South America -- Description and travel; Voyag...     G   \n",
       "22421  South America -- Description and travel; Geolo...     G   \n",
       "24675  South America -- Description and travel; Geolo...     G   \n",
       "0      United States -- History -- Revolution, 1775-1...  E201   \n",
       "...                                                  ...   ...   \n",
       "75210                                                NaN   nan   \n",
       "75211                                                NaN   nan   \n",
       "75212                                                NaN   nan   \n",
       "75213                                                NaN   nan   \n",
       "75214                                                NaN   nan   \n",
       "\n",
       "                                             Bookshelves LoCC2 LoCC3   LoCC4  \\\n",
       "12195  United States; DE Sachbuch; Browsing: History ...    DK    DL    E151   \n",
       "6280   South America; Browsing: History - General; Br...    QE    QH   F2201   \n",
       "22421  German Language Books; DE Sachbuch; Browsing: ...    QE    QH   F2201   \n",
       "24675  DE Sachbuch; Browsing: History - General; Brow...    QE    QH   F2201   \n",
       "0      Politics; American Revolutionary War; United S...    JK  None    None   \n",
       "...                                                  ...   ...   ...     ...   \n",
       "75210                                                NaN  None  None    None   \n",
       "75211                                                NaN  None  None    None   \n",
       "75212                                                NaN  None  None    None   \n",
       "75213                                                NaN  None  None    None   \n",
       "75214                                                NaN  None  None    None   \n",
       "\n",
       "        LoCC5  \n",
       "12195    None  \n",
       "6280    F2301  \n",
       "22421   F2301  \n",
       "24675   F2301  \n",
       "0        None  \n",
       "...       ...  \n",
       "75210    None  \n",
       "75211    None  \n",
       "75212    None  \n",
       "75213    None  \n",
       "75214    None  \n",
       "\n",
       "[75215 rows x 13 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted_alphabetical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d16fd57a-3727-48d4-be37-1e436d28bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This counts the total number of times a LoC classification label appears across all\n",
    "# columns (LoCC, LoCC2, etc). It filters for English-language only; otherwise it includes\n",
    "# all types of LCC labels (A, A123, AB, AB123, ABC, ABC123, etc)\n",
    "# The output is label_counts\n",
    "\n",
    "# Initialize a Counter to tally label instances\n",
    "label_counts = Counter()\n",
    "\n",
    "# Iterate through the rows of the DataFrame where Language is \"en\"\n",
    "filtered_df = df[df[\"Language\"].fillna(\"\").astype(str).str.strip() == \"en\"]\n",
    "\n",
    "# Count occurrences of each label in LoCC, LoCC2, LoCC3, LoCC4, and LoCC5 columns\n",
    "for col in [\"LoCC\", \"LoCC2\", \"LoCC3\", \"LoCC4\", \"LoCC5\"]:\n",
    "    label_counts.update(filtered_df[col].fillna(\"\").astype(str).str.strip())\n",
    "\n",
    "# Filter out empty strings from the count\n",
    "if \"\" in label_counts:\n",
    "    del label_counts[\"\"]\n",
    "\n",
    "# Create a list of labels with more than 100 instances\n",
    "# labels_list = [label for label, count in label_counts.items() if count > 100]\n",
    "\n",
    "# Output the list of popular labels and their counts (optional)\n",
    "# print(\"Labels with More Than 100 Instances:\", labels_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6ecc818-91b5-4c5a-9586-3570ff080fcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'PS': 11658,\n",
       "         'PR': 9902,\n",
       "         'PZ': 7297,\n",
       "         'AP': 2428,\n",
       "         'DA': 1705,\n",
       "         'PQ': 1421,\n",
       "         'PN': 916,\n",
       "         'DS': 792,\n",
       "         'PT': 783,\n",
       "         'D501': 782,\n",
       "         'BX': 715,\n",
       "         'BV': 658,\n",
       "         'QH': 650,\n",
       "         'QL': 646,\n",
       "         'BS': 627,\n",
       "         'DC': 548,\n",
       "         'E456': 546,\n",
       "         'Z': 527,\n",
       "         'E011': 508,\n",
       "         'D': 499,\n",
       "         'E151': 454,\n",
       "         'E300': 448,\n",
       "         'G': 425,\n",
       "         'BF': 409,\n",
       "         'TX': 409,\n",
       "         'BL': 394,\n",
       "         'HV': 379,\n",
       "         'DT': 375,\n",
       "         'HQ': 366,\n",
       "         'PA': 348,\n",
       "         'B': 343,\n",
       "         'ML': 309,\n",
       "         'GV': 281,\n",
       "         'F1001': 276,\n",
       "         'PG': 272,\n",
       "         'BJ': 255,\n",
       "         'AG': 254,\n",
       "         'GR': 253,\n",
       "         'ND': 249,\n",
       "         'DG': 249,\n",
       "         'PE': 247,\n",
       "         'CT': 241,\n",
       "         'SB': 235,\n",
       "         'SF': 228,\n",
       "         'BR': 226,\n",
       "         'nan': 222,\n",
       "         'DU': 208,\n",
       "         'Q': 204,\n",
       "         'M': 195,\n",
       "         'RC': 194,\n",
       "         'T': 192,\n",
       "         'BT': 191,\n",
       "         'QC': 189,\n",
       "         'NC': 188,\n",
       "         'QK': 188,\n",
       "         'F850.5': 185,\n",
       "         'HD': 183,\n",
       "         'N': 179,\n",
       "         'TT': 176,\n",
       "         'F106': 171,\n",
       "         'F590.3': 167,\n",
       "         'LB': 163,\n",
       "         'QE': 163,\n",
       "         'F001': 160,\n",
       "         'NK': 158,\n",
       "         'HE': 157,\n",
       "         'QB': 154,\n",
       "         'QP': 145,\n",
       "         'QA': 141,\n",
       "         'AE': 140,\n",
       "         'BP': 138,\n",
       "         'DH': 133,\n",
       "         'F206': 131,\n",
       "         'TS': 128,\n",
       "         'GN': 127,\n",
       "         'E201': 123,\n",
       "         'HX': 119,\n",
       "         'PJ': 118,\n",
       "         'JK': 117,\n",
       "         'SK': 116,\n",
       "         'RA': 116,\n",
       "         'S': 116,\n",
       "         'DK': 113,\n",
       "         'TL': 106,\n",
       "         'DP': 106,\n",
       "         'TK': 102,\n",
       "         'DD': 102,\n",
       "         'E660': 101,\n",
       "         'GT': 98,\n",
       "         'HF': 94,\n",
       "         'UA': 94,\n",
       "         'DF': 89,\n",
       "         'PK': 87,\n",
       "         'KD': 87,\n",
       "         'KF': 86,\n",
       "         'DR': 85,\n",
       "         'R': 84,\n",
       "         'RM': 82,\n",
       "         'HB': 81,\n",
       "         'U': 79,\n",
       "         'F786': 79,\n",
       "         'TP': 79,\n",
       "         'QD': 77,\n",
       "         'F721': 75,\n",
       "         'MT': 75,\n",
       "         'HG': 72,\n",
       "         'J': 72,\n",
       "         'TJ': 65,\n",
       "         'JC': 62,\n",
       "         'DL': 62,\n",
       "         'PL': 60,\n",
       "         'HN': 60,\n",
       "         'HT': 59,\n",
       "         'F1201': 59,\n",
       "         'SH': 58,\n",
       "         'E740': 55,\n",
       "         'F350.5': 55,\n",
       "         'LC': 54,\n",
       "         'TA': 52,\n",
       "         'LA': 51,\n",
       "         'F516': 49,\n",
       "         'HM': 48,\n",
       "         'PH': 48,\n",
       "         'TN': 47,\n",
       "         'HC': 45,\n",
       "         'NE': 45,\n",
       "         'PB': 44,\n",
       "         'F396': 44,\n",
       "         'CB': 43,\n",
       "         'TH': 42,\n",
       "         'JX': 41,\n",
       "         'E186': 40,\n",
       "         'CR': 39,\n",
       "         'AC': 38,\n",
       "         'TR': 37,\n",
       "         'LF': 36,\n",
       "         'F2201': 36,\n",
       "         'D731': 35,\n",
       "         'UG': 35,\n",
       "         'BM': 35,\n",
       "         'TC': 35,\n",
       "         'BD': 35,\n",
       "         'VM': 35,\n",
       "         'RD': 35,\n",
       "         'DQ': 35,\n",
       "         'F1401': 34,\n",
       "         'RG': 34,\n",
       "         'F296': 34,\n",
       "         'KZ': 34,\n",
       "         'HS': 33,\n",
       "         'DB': 33,\n",
       "         'TF': 32,\n",
       "         'VK': 32,\n",
       "         'NB': 31,\n",
       "         'PM': 31,\n",
       "         'K': 30,\n",
       "         'JN': 29,\n",
       "         'NA': 29,\n",
       "         'TD': 27,\n",
       "         'BC': 24,\n",
       "         'F476': 24,\n",
       "         'V': 24,\n",
       "         'PC': 23,\n",
       "         'F1751': 23,\n",
       "         'SD': 23,\n",
       "         'JV': 23,\n",
       "         'F2501': 22,\n",
       "         'P': 22,\n",
       "         'F2801': 22,\n",
       "         'GB': 21,\n",
       "         'DE': 19,\n",
       "         'F3401': 18,\n",
       "         'BQ': 18,\n",
       "         'F2155': 18,\n",
       "         'DX': 17,\n",
       "         'UF': 17,\n",
       "         'LD': 17,\n",
       "         'CS': 17,\n",
       "         'RJ': 16,\n",
       "         'RS': 16,\n",
       "         'DJ': 15,\n",
       "         'H': 15,\n",
       "         'QM': 15,\n",
       "         'HJ': 13,\n",
       "         'QR': 13,\n",
       "         'JF': 13,\n",
       "         'UD': 13,\n",
       "         'UH': 13,\n",
       "         'BH': 12,\n",
       "         'VA': 12,\n",
       "         'RT': 11,\n",
       "         'UB': 11,\n",
       "         'GC': 11,\n",
       "         'F1561': 10,\n",
       "         'F1900': 10,\n",
       "         'LT': 10,\n",
       "         'GA': 10,\n",
       "         'UC': 9,\n",
       "         'RX': 8,\n",
       "         'CC': 8,\n",
       "         'UE': 8,\n",
       "         'AZ': 7,\n",
       "         'F2001': 7,\n",
       "         'F2351': 7,\n",
       "         'RZ': 7,\n",
       "         'RE': 7,\n",
       "         'RK': 7,\n",
       "         'F1601': 7,\n",
       "         'LH': 7,\n",
       "         'KJ': 6,\n",
       "         'RB': 6,\n",
       "         'PF': 6,\n",
       "         'CJ': 6,\n",
       "         'TE': 6,\n",
       "         'F3051': 6,\n",
       "         'JQ': 5,\n",
       "         'JS': 5,\n",
       "         'GF': 5,\n",
       "         'E895': 5,\n",
       "         'TG': 5,\n",
       "         'F1861': 5,\n",
       "         'HA': 4,\n",
       "         'F2661': 4,\n",
       "         'NX': 4,\n",
       "         'PD': 4,\n",
       "         'F1951': 4,\n",
       "         'JA': 4,\n",
       "         'L': 4,\n",
       "         'AS': 4,\n",
       "         'RF': 4,\n",
       "         'F2701': 4,\n",
       "         'F1461': 4,\n",
       "         'AM': 4,\n",
       "         'F3301': 4,\n",
       "         'F2301': 4,\n",
       "         'AY': 4,\n",
       "         'JZ': 4,\n",
       "         'F975': 3,\n",
       "         'KL': 3,\n",
       "         'VF': 3,\n",
       "         'F2251': 3,\n",
       "         'CE': 3,\n",
       "         'E838': 2,\n",
       "         'DJK': 2,\n",
       "         'JL': 2,\n",
       "         'QH301': 2,\n",
       "         'RL': 2,\n",
       "         'F2131': 2,\n",
       "         'VB': 2,\n",
       "         'VE': 2,\n",
       "         'F1521': 2,\n",
       "         'CD': 2,\n",
       "         'KU': 2,\n",
       "         'KNX': 2,\n",
       "         'KDZ': 1,\n",
       "         'KE': 1,\n",
       "         'F3701': 1,\n",
       "         'KN': 1,\n",
       "         'F1501': 1,\n",
       "         'LE': 1,\n",
       "         'CN': 1,\n",
       "         'F1481': 1,\n",
       "         'F1541': 1,\n",
       "         'KBR': 1,\n",
       "         'VG': 1,\n",
       "         'RV': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bade9aed-cf5e-493d-9a5e-25fb19b87ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62731\n"
     ]
    }
   ],
   "source": [
    "total_count = sum(label_counts.values())\n",
    "print(total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3637cdb5-3fc1-425c-a2fe-17bf4f9ffb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'PS': 11658, 'PR': 9902, 'PZ': 7297, 'AP': 2428, 'DA': 1705, 'PQ': 1421, 'PN': 916, 'DS': 792, 'PT': 783, 'BX': 715, 'BV': 658, 'QH': 652, 'QL': 646, 'BS': 627, 'DC': 548, 'BF': 409, 'TX': 409, 'BL': 394, 'HV': 379, 'DT': 375, 'HQ': 366, 'PA': 348, 'ML': 309, 'GV': 281, 'PG': 272, 'BJ': 255, 'AG': 254, 'GR': 253, 'ND': 249, 'DG': 249, 'PE': 247, 'CT': 241, 'SB': 235, 'SF': 228, 'BR': 226, 'na': 222, 'DU': 208, 'RC': 194, 'BT': 191, 'QC': 189, 'NC': 188, 'QK': 188, 'HD': 183, 'TT': 176, 'LB': 163, 'QE': 163, 'NK': 158, 'HE': 157, 'QB': 154, 'QP': 145, 'QA': 141, 'AE': 140, 'BP': 138, 'DH': 133, 'TS': 128, 'GN': 127, 'HX': 119, 'PJ': 118, 'JK': 117, 'SK': 116, 'RA': 116, 'DK': 113, 'TL': 106, 'DP': 106, 'TK': 102, 'DD': 102, 'GT': 98, 'HF': 94, 'UA': 94, 'DF': 89, 'KD': 88, 'PK': 87, 'KF': 86, 'DR': 85, 'RM': 82, 'HB': 81, 'TP': 79, 'QD': 77, 'MT': 75, 'HG': 72, 'TJ': 65, 'JC': 62, 'DL': 62, 'PL': 60, 'HN': 60, 'HT': 59, 'SH': 58, 'LC': 54, 'TA': 52, 'LA': 51, 'HM': 48, 'PH': 48, 'TN': 47, 'HC': 45, 'NE': 45, 'PB': 44, 'CB': 43, 'TH': 42, 'JX': 41, 'CR': 39, 'AC': 38, 'TR': 37, 'LF': 36, 'UG': 35, 'BM': 35, 'TC': 35, 'BD': 35, 'VM': 35, 'RD': 35, 'DQ': 35, 'RG': 34, 'KZ': 34, 'HS': 33, 'DB': 33, 'TF': 32, 'VK': 32, 'NB': 31, 'PM': 31, 'JN': 29, 'NA': 29, 'TD': 27, 'BC': 24, 'PC': 23, 'SD': 23, 'JV': 23, 'GB': 21, 'DE': 19, 'BQ': 18, 'DX': 17, 'UF': 17, 'LD': 17, 'CS': 17, 'DJ': 17, 'RJ': 16, 'RS': 16, 'QM': 15, 'HJ': 13, 'QR': 13, 'JF': 13, 'UD': 13, 'UH': 13, 'BH': 12, 'VA': 12, 'RT': 11, 'UB': 11, 'GC': 11, 'LT': 10, 'GA': 10, 'UC': 9, 'RX': 8, 'CC': 8, 'UE': 8, 'AZ': 7, 'RZ': 7, 'RE': 7, 'RK': 7, 'LH': 7, 'KJ': 6, 'RB': 6, 'PF': 6, 'CJ': 6, 'TE': 6, 'JQ': 5, 'JS': 5, 'GF': 5, 'TG': 5, 'HA': 4, 'NX': 4, 'PD': 4, 'JA': 4, 'AS': 4, 'RF': 4, 'AM': 4, 'AY': 4, 'JZ': 4, 'KL': 3, 'VF': 3, 'KN': 3, 'CE': 3, 'JL': 2, 'RL': 2, 'VB': 2, 'VE': 2, 'CD': 2, 'KU': 2, 'KE': 1, 'LE': 1, 'CN': 1, 'KB': 1, 'VG': 1, 'RV': 1})\n"
     ]
    }
   ],
   "source": [
    "# Next, we are going to re-count the labels using the following criteria\n",
    "# 1. Exclude rows that contain a single letter followed by numbers (D501 and E011 will be excluded)\n",
    "# 2. Exclude rows that contain only 1 letter (Z will be excluded)\n",
    "# 3. Include rows that start with 2 letters (PS, DJ, and DJK will be included)\n",
    "# 4. Combine the count for rows that start with the same first 2 characters (DJK will be added to DJ; QH301 added to QH)\n",
    "# The output is aggregated_counts\n",
    "\n",
    "# Filter the counts based on the specified conditions\n",
    "filtered_counts = Counter({\n",
    "    label: count for label, count in label_counts.items()\n",
    "    if len(label) >= 2 and label[:2].isalpha() and not any(char.isdigit() for char in label[:2])  # Handle first two characters correctly\n",
    "})\n",
    "\n",
    "# Aggregate counts by the first two characters\n",
    "aggregated_counts = Counter()\n",
    "for label, count in filtered_counts.items():\n",
    "    prefix = label[:2]  # Take the first two characters\n",
    "    aggregated_counts[prefix] += count\n",
    "\n",
    "# Print the aggregated counts\n",
    "print(aggregated_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43aa9202-fa8b-41b7-a192-44cd735313c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54850\n"
     ]
    }
   ],
   "source": [
    "total_aggregated_count = sum(aggregated_counts.values())\n",
    "print(total_aggregated_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfa8965e-f719-420e-b56a-1abf69236ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'KN': 2, 'DJ': 2, 'QH': 2, 'KD': 1, 'DL': 0, 'HD': 0, 'JV': 0, 'AP': 0, 'KZ': 0, 'JN': 0, 'KL': 0, 'BF': 0, 'BC': 0, 'RF': 0, 'QA': 0, 'AS': 0, 'RB': 0, 'CN': 0, 'BJ': 0, 'LE': 0, 'NK': 0, 'ML': 0, 'LF': 0, 'BH': 0, 'JF': 0, 'RL': 0, 'DQ': 0, 'JS': 0, 'GA': 0, 'PJ': 0, 'PS': 0, 'CJ': 0, 'GV': 0, 'RE': 0, 'QK': 0, 'DC': 0, 'QR': 0, 'RV': 0, 'TN': 0, 'RM': 0, 'JL': 0, 'RA': 0, 'TG': 0, 'HV': 0, 'GF': 0, 'HB': 0, 'DB': 0, 'RC': 0, 'VG': 0, 'BS': 0, 'DF': 0, 'QE': 0, 'PG': 0, 'AE': 0, 'PK': 0, 'QL': 0, 'VF': 0, 'NB': 0, 'NC': 0, 'VE': 0, 'RD': 0, 'LB': 0, 'HM': 0, 'CR': 0, 'GT': 0, 'DU': 0, 'BV': 0, 'DA': 0, 'TS': 0, 'CC': 0, 'PT': 0, 'SK': 0, 'TC': 0, 'TE': 0, 'RK': 0, 'CS': 0, 'BM': 0, 'QM': 0, 'TA': 0, 'HF': 0, 'BP': 0, 'TR': 0, 'GB': 0, 'PF': 0, 'ND': 0, 'HN': 0, 'PL': 0, 'HC': 0, 'PM': 0, 'DD': 0, 'UA': 0, 'RZ': 0, 'CB': 0, 'TX': 0, 'CT': 0, 'AC': 0, 'RJ': 0, 'TT': 0, 'UC': 0, 'UE': 0, 'LH': 0, 'PH': 0, 'KJ': 0, 'PN': 0, 'PQ': 0, 'BR': 0, 'DT': 0, 'DE': 0, 'DK': 0, 'BX': 0, 'LD': 0, 'KF': 0, 'HA': 0, 'HX': 0, 'SB': 0, 'LC': 0, 'BT': 0, 'SF': 0, 'PA': 0, 'TJ': 0, 'HQ': 0, 'LT': 0, 'CD': 0, 'HS': 0, 'PZ': 0, 'DH': 0, 'NE': 0, 'GN': 0, 'JX': 0, 'DP': 0, 'CE': 0, 'JK': 0, 'RG': 0, 'VB': 0, 'JZ': 0, 'UD': 0, 'PC': 0, 'UH': 0, 'DS': 0, 'TH': 0, 'UF': 0, 'QD': 0, 'SH': 0, 'RT': 0, 'TK': 0, 'TF': 0, 'AY': 0, 'DX': 0, 'TP': 0, 'GR': 0, 'PD': 0, 'AZ': 0, 'QC': 0, 'KE': 0, 'LA': 0, 'VA': 0, 'PR': 0, 'DG': 0, 'MT': 0, 'TD': 0, 'VK': 0, 'KU': 0, 'JC': 0, 'UB': 0, 'HG': 0, 'TL': 0, 'BL': 0, 'HE': 0, 'PE': 0, 'PB': 0, 'JA': 0, 'QB': 0, 'HT': 0, 'UG': 0, 'HJ': 0, 'BD': 0, 'SD': 0, 'AM': 0, 'QP': 0, 'JQ': 0, 'BQ': 0, 'RX': 0, 'VM': 0, 'DR': 0, 'NA': 0, 'AG': 0, 'GC': 0, 'RS': 0, 'NX': 0})\n"
     ]
    }
   ],
   "source": [
    "# Now we are going to subtract aggregated_counts from label_counts\n",
    "# This will tell us how many texts were added into aggregated_counts\n",
    "\n",
    "# Perform subtraction on keys that are present in both dictionaries\n",
    "common_keys = set(aggregated_counts.keys()) & set(label_counts.keys())  # Intersection of keys\n",
    "\n",
    "# Subtract counts for common keys\n",
    "difference_counts = Counter({\n",
    "    key: aggregated_counts[key] - label_counts[key] for key in common_keys\n",
    "})\n",
    "\n",
    "# Print the resulting difference counts\n",
    "# For example, 'QH': 2 means that 2 additional texts were added to QH\n",
    "print(difference_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a291b71c-fb1c-4ac2-b476-2042e711da76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KDZ: 1\n",
      "nan: 222\n",
      "DJK: 2\n",
      "QH301: 2\n",
      "KNX: 2\n",
      "KBR: 1\n"
     ]
    }
   ],
   "source": [
    "# Confirm the count using a different approach: how many texts are there from the original label_counts that\n",
    "# start with 2 letters (PS) AND contain more 2 characters (PS123 or PSX)\n",
    "\n",
    "# Filter and print rows that start with 2 letters, contain more than 2 characters, and don't start with numbers\n",
    "filtered_rows = {\n",
    "    label: count for label, count in label_counts.items()\n",
    "    if len(label) > 2 and label[:2].isalpha() and not label[0].isdigit()\n",
    "}\n",
    "\n",
    "# Print the filtered rows\n",
    "# This should correspond with the above difference_counts; QH301: 2 corresponds with QH: 2. KDZ: 1 corresponds with KD: 1, etc\n",
    "for label, count in filtered_rows.items():\n",
    "    print(f\"{label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86f9929c-eac0-4cec-9465-5ea6eb75eefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JK', 'BS', 'PR', 'PS', 'BX', 'PA', 'PE', 'TK', 'PZ', 'QA', 'HX', 'PQ', 'HV', 'NC', 'TL', 'PJ', 'BR', 'BL', 'AE', 'PG', 'AG', 'PT', 'QC', 'DS', 'DA', 'BJ', 'PN', 'CT', 'QH', 'DP', 'GV', 'BF', 'LB', 'ND', 'QL', 'DU', 'BV', 'DG', 'HQ', 'ML', 'DT', 'TX', 'AP', 'BT', 'QK', 'HD', 'DC', 'DK', 'SK', 'QE', 'RC', 'DD', 'QB', 'GR', 'SB', 'BP', 'GN', 'QP', 'HE', 'SF', 'DH', 'RA', 'TS', 'TT', 'NK']\n"
     ]
    }
   ],
   "source": [
    "# Create a list with the categories containing more than 100 texts (and exclude unclassified NaN/na texts)\n",
    "# This list will be used to produce the dataframes\n",
    "# (Instead of hard-coding the two-character categories, create the list from the data)\n",
    "\n",
    "filtered_keys = [key for key, count in aggregated_counts.items() if count >= 100 and key.lower() != \"na\"]\n",
    "\n",
    "# Print the list of filtered keys\n",
    "print(filtered_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4024f7c-c381-45db-b060-bf57ee65a23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the number of categories to included in the analysis\n",
    "\n",
    "len(filtered_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4158bf68-2876-4f1a-9c7a-651739397826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to store new dataframes. There will be a new dataframe for each LCC label, and it \n",
    "# will contain the metadata for all texts with that label\n",
    "\n",
    "label_dataframes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4764ebea-e240-406b-8fba-28fa6bb85997",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JK with 117 rows.\n",
      "Saved BS with 627 rows.\n",
      "Saved PR with 9902 rows.\n",
      "Saved PS with 11658 rows.\n",
      "Saved BX with 715 rows.\n",
      "Saved PA with 348 rows.\n",
      "Saved PE with 247 rows.\n",
      "Saved TK with 102 rows.\n",
      "Saved PZ with 7297 rows.\n",
      "Saved QA with 141 rows.\n",
      "Saved HX with 119 rows.\n",
      "Saved PQ with 1421 rows.\n",
      "Saved HV with 379 rows.\n",
      "Saved NC with 188 rows.\n",
      "Saved TL with 106 rows.\n",
      "Saved PJ with 118 rows.\n",
      "Saved BR with 226 rows.\n",
      "Saved BL with 394 rows.\n",
      "Saved AE with 140 rows.\n",
      "Saved PG with 272 rows.\n",
      "Saved AG with 254 rows.\n",
      "Saved PT with 783 rows.\n",
      "Saved QC with 189 rows.\n",
      "Saved DS with 792 rows.\n",
      "Saved DA with 1705 rows.\n",
      "Saved BJ with 255 rows.\n",
      "Saved PN with 916 rows.\n",
      "Saved CT with 241 rows.\n",
      "Saved QH with 650 rows.\n",
      "Saved DP with 106 rows.\n",
      "Saved GV with 281 rows.\n",
      "Saved BF with 409 rows.\n",
      "Saved LB with 163 rows.\n",
      "Saved ND with 249 rows.\n",
      "Saved QL with 646 rows.\n",
      "Saved DU with 208 rows.\n",
      "Saved BV with 658 rows.\n",
      "Saved DG with 249 rows.\n",
      "Saved HQ with 366 rows.\n",
      "Saved ML with 309 rows.\n",
      "Saved DT with 375 rows.\n",
      "Saved TX with 409 rows.\n",
      "Saved AP with 2428 rows.\n",
      "Saved BT with 191 rows.\n",
      "Saved QK with 188 rows.\n",
      "Saved HD with 183 rows.\n",
      "Saved DC with 548 rows.\n",
      "Saved DK with 113 rows.\n",
      "Saved SK with 116 rows.\n",
      "Saved QE with 163 rows.\n",
      "Saved RC with 194 rows.\n",
      "Saved DD with 102 rows.\n",
      "Saved QB with 154 rows.\n",
      "Saved GR with 253 rows.\n",
      "Saved SB with 235 rows.\n",
      "Saved BP with 138 rows.\n",
      "Saved GN with 127 rows.\n",
      "Saved QP with 145 rows.\n",
      "Saved HE with 157 rows.\n",
      "Saved SF with 228 rows.\n",
      "Saved DH with 133 rows.\n",
      "Saved RA with 116 rows.\n",
      "Saved TS with 128 rows.\n",
      "Saved TT with 176 rows.\n",
      "Saved NK with 158 rows.\n"
     ]
    }
   ],
   "source": [
    "# For each LCC label in filtered_keys, go through the PG metadata and look for instances where:\n",
    "# 1. The LCC label is contained in one of the columns\n",
    "# 2. Get the text of only those where the language is English \n",
    "# If a row matches these criteria, store it in a new dataframe whose name is the LCC label (eg \"AB\")\n",
    "# At the end, there will be a dictionary containing 65 dataframes, one dataframe for each label\n",
    "# that contains more than 100 texts. And the dataframe will contain the metadata for all 100+ records \n",
    "# corresponding to that LCC label\n",
    "\n",
    "matching_rows_count = 0\n",
    "for label in filtered_keys:\n",
    "    # filter rows where label matches either \"LoCC\", \"LoCC2\", ..., and where \"Language\" is \"en\"\n",
    "    matching_rows = df[\n",
    "        ((df[\"LoCC\"].fillna(\"\").astype(str).str.strip().eq(label)) |\n",
    "         (df[\"LoCC2\"].fillna(\"\").astype(str).str.strip().eq(label)) | \n",
    "         (df[\"LoCC3\"].fillna(\"\").astype(str).str.strip().eq(label)) |\n",
    "         (df[\"LoCC4\"].fillna(\"\").astype(str).str.strip().eq(label)) |\n",
    "         (df[\"LoCC5\"].fillna(\"\").astype(str).str.strip().eq(label))) &\n",
    "        (df[\"Language\"].fillna(\"\").astype(str).str.strip() == \"en\")\n",
    "    ]\n",
    "    matching_rows_count +=  len(matching_rows)\n",
    "\n",
    "\n",
    "    \n",
    "    # Store the resulting dataframe in a dictionary\n",
    "    label_dataframes[label] = matching_rows\n",
    "\n",
    "    # Print confirmation\n",
    "    print(f\"Saved {label} with {len(matching_rows)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b6fd611-0cf0-4e16-a17b-c29293f23de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total texts: 51104\n",
      "Number of texts with label \"QH\": 650\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of total texts: {matching_rows_count}')\n",
    "print(f'Number of texts with label \"QH\": {len(label_dataframes[\"QH\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45028084-ba39-416a-aab6-0bd7b0e98b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the two addition QH301 texts are included\n",
    "qh301_rows = df[\n",
    "    (df[\"LoCC\"].fillna(\"\").astype(str).str.strip() == \"QH301\") &\n",
    "    (df[\"Language\"].fillna(\"\").astype(str).str.strip() == \"en\")\n",
    "]\n",
    "\n",
    "# Ensure QH301 texts are added to QH\n",
    "if \"QH\" in label_dataframes:\n",
    "    label_dataframes[\"QH\"] = pd.concat([label_dataframes[\"QH\"], qh301_rows], ignore_index=True)\n",
    "else:\n",
    "    label_dataframes[\"QH\"] = qh301_rows\n",
    "\n",
    "matching_rows_count += len(qh301_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbfc3c24-0782-4df9-884d-71f8e03e9e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total texts 51106\n",
      "Number of texts with label \"QH\": 652\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of total texts: {matching_rows_count}')\n",
    "print(f'Number of texts with label \"QH\": {len(label_dataframes[\"QH\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1aa93a36-0262-460c-a6ed-ec294e806ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next steps will add a column to each dataframes that contain the full texts.\n",
    "# This is going to make the dataframes very large!\n",
    "\n",
    "# First, define the base file path for where the full text files are stored on local computer\n",
    "# I.e., define the location of wd/data/pg-texts/\n",
    "# Note: for Macs, Jupyter Lab prefers the full file path rather than relative path\n",
    "# I.e. don't use ~/Desktop/.../\n",
    "\n",
    "BASE_PATH = \"/path/to/pg-texts\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d028df7-ad39-4363-ab45-ef9b56d99940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for TF-IDF analysis\n",
    "# Define a function that will:\n",
    "# 1. Read full text files from local computer, and\n",
    "# 2. Perform text preprocessing: for TF-IDF, it is \n",
    "#   a) To remove front & end matter from the full text files\n",
    "#   b) Convert text to lowercase\n",
    "#   c) Remove punctuation\n",
    "\n",
    "\n",
    "def load_and_clean_text(text_id):\n",
    "    file_path = os.path.join(BASE_PATH, str(text_id), f\"pg{text_id}.txt\")\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            full_text = file.read()\n",
    "\n",
    "        # Define regex patterns for front and end matter\n",
    "        start_pattern = r\"\\*\\*\\* START OF (?:THIS|THE) PROJECT GUTENBERG EBOOK .* \\*\\*\\*\"\n",
    "        end_pattern = r\"\\*\\*\\* END OF (?:THIS|THE) PROJECT GUTENBERG EBOOK .* \\*\\*\\*\"\n",
    "\n",
    "        # Remove front matter\n",
    "        start_match = re.search(start_pattern, full_text)\n",
    "        if start_match:\n",
    "            full_text = full_text[start_match.end():].strip()\n",
    "\n",
    "        # Remove end matter\n",
    "        end_match = re.search(end_pattern, full_text)\n",
    "        if end_match:\n",
    "            full_text = full_text[:end_match.start()].strip()\n",
    "\n",
    "        # Ensure full_text is still a valid string\n",
    "        if not isinstance(full_text, str):\n",
    "            raise ValueError(\"Processed text is not a valid string.\")\n",
    "\n",
    "        # Convert text to lowercase\n",
    "        full_text = full_text.lower()\n",
    "\n",
    "        # Remove punctuation using regex (keeps alphanumeric characters and spaces)\n",
    "        full_text = re.sub(r\"[^\\w\\s]\", \"\", full_text)\n",
    "\n",
    "        # Format text for CSV: Escape double quotes and replace newlines\n",
    "        cleaned_text = full_text.replace('\"', '\"\"').replace(\"\\n\", \" \")\n",
    "\n",
    "        return cleaned_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa74c9c0-b42f-4119-8197-d66073bbc995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing label: JK\n",
      "Processing label: BS\n",
      "Processing label: PR\n",
      "Processing label: PS\n",
      "Processing label: BX\n",
      "Processing label: PA\n",
      "Processing label: PE\n",
      "Processing label: TK\n",
      "Processing label: PZ\n",
      "Processing label: QA\n",
      "Processing label: HX\n",
      "Processing label: PQ\n",
      "Processing label: HV\n",
      "Processing label: NC\n",
      "Processing label: TL\n",
      "Processing label: PJ\n",
      "Processing label: BR\n",
      "Processing label: BL\n",
      "Processing label: AE\n",
      "Processing label: PG\n",
      "Processing label: AG\n",
      "Processing label: PT\n",
      "Processing label: QC\n",
      "Processing label: DS\n",
      "Processing label: DA\n",
      "Processing label: BJ\n",
      "Processing label: PN\n",
      "Processing label: CT\n",
      "Processing label: QH\n",
      "Processing label: DP\n",
      "Processing label: GV\n",
      "Processing label: BF\n",
      "Processing label: LB\n",
      "Processing label: ND\n",
      "Processing label: QL\n",
      "Processing label: DU\n",
      "Processing label: BV\n",
      "Processing label: DG\n",
      "Processing label: HQ\n",
      "Processing label: ML\n",
      "Processing label: DT\n",
      "Processing label: TX\n",
      "Processing label: AP\n",
      "Processing label: BT\n",
      "Processing label: QK\n",
      "Processing label: HD\n",
      "Processing label: DC\n",
      "Processing label: DK\n",
      "Processing label: SK\n",
      "Processing label: QE\n",
      "Processing label: RC\n",
      "Processing label: DD\n",
      "Processing label: QB\n",
      "Processing label: GR\n",
      "Processing label: SB\n",
      "Processing label: BP\n",
      "Processing label: GN\n",
      "Processing label: QP\n",
      "Processing label: HE\n",
      "Processing label: SF\n",
      "Processing label: DH\n",
      "Processing label: RA\n",
      "Processing label: TS\n",
      "Processing label: TT\n",
      "Processing label: NK\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each dataframe in the dictionary and read/clean/load data\n",
    "\n",
    "for label, df in label_dataframes.items():    \n",
    "    print(f\"Processing label: {label}\")\n",
    "\n",
    "    # Apply the function to load and clean texts to each row in the \"Text#\" column and store \n",
    "    # in a new column called \"FullText\"\n",
    "    df[\"FullText\"] = df[\"Text#\"].astype(str).apply(load_and_clean_text)\n",
    "\n",
    "    # Save the updated dataframe back to the dictionary\n",
    "    label_dataframes[label] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f5b05c7-2055-4d18-bbfc-7d7010ff946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/path/to/dataPG_TF-IDF.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_dataframes, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
